{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn EDA\n",
    "\n",
    "This notebook performs exploratory data analysis (EDA) on the same synthetic\n",
    "telecom churn dataset used by the training pipeline in this repository.\n",
    "\n",
    "- Data generation code lives in `training/preprocess.py`.\n",
    "- The model training script lives in `training/train.py`.\n",
    "\n",
    "The goal is to show:\n",
    "\n",
    "- Overall dataset structure\n",
    "- Churn rate distribution\n",
    "- Feature distributions and relationships with churn\n",
    "- Simple correlations and baseline model behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from training.preprocess import (\n",
    "    FEATURE_COLUMNS,\n",
    "    TARGET_COLUMN,\n",
    "    generate_synthetic_churn_data,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the same synthetic dataset used by training\n",
    "df = generate_synthetic_churn_data(n_samples=2000, random_state=42)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn rate distribution\n",
    "\n",
    "The target column is `churn` (1 = churned, 0 = retained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_rate = df[\"churn\"].mean()\n",
    "print(f\"Overall churn rate: {churn_rate:.3f}\")\n",
    "\n",
    "ax = sns.countplot(x=\"churn\", data=df)\n",
    "ax.set_xticklabels([\"Retained (0)\", \"Churned (1)\"])\n",
    "ax.set_title(\"Churn vs. Retained counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature distributions\n",
    "\n",
    "We inspect the distributions of key numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"tenure\", \"monthly_charges\", \"support_calls\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(numeric_cols), figsize=(15, 4))\n",
    "for ax, col in zip(axes, numeric_cols):\n",
    "    sns.histplot(df[col], bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(f\"Distribution of {col}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn rate by key segments\n",
    "\n",
    "The synthetic generation code includes several drivers:\n",
    "\n",
    "- Higher `monthly_charges`\n",
    "- Shorter `tenure`\n",
    "- `contract_type` = 0 (month-to-month)\n",
    "- Higher `support_calls`\n",
    "- `is_senior` = 1\n",
    "\n",
    "We can verify these patterns by looking at segment-level churn rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segment_churn_rate(df, by, title):\n",
    "    rates = df.groupby(by)[\"churn\"].mean().reset_index()\n",
    "    ax = sns.barplot(x=by, y=\"churn\", data=rates)\n",
    "    ax.set_ylabel(\"Churn rate\")\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_segment_churn_rate(df, \"contract_type\", \"Churn rate by contract type\")\n",
    "plot_segment_churn_rate(df, \"has_internet\", \"Churn rate by internet service\")\n",
    "plot_segment_churn_rate(df, \"is_senior\", \"Churn rate by senior citizen flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn rate by binned tenure\n",
    "df[\"tenure_bin\"] = pd.cut(df[\"tenure\"], bins=[0, 6, 12, 24, 48, 72])\n",
    "plot_segment_churn_rate(df, \"tenure_bin\", \"Churn rate by tenure bin\")\n",
    "\n",
    "# Churn rate by binned monthly charges\n",
    "df[\"monthly_bin\"] = pd.cut(df[\"monthly_charges\"], bins=[20, 40, 60, 80, 100, 120])\n",
    "plot_segment_churn_rate(df, \"monthly_bin\", \"Churn rate by monthly charges bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "We compute a simple correlation matrix for the features and the churn label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[FEATURE_COLUMNS + [TARGET_COLUMN]].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Correlation heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model check (optional)\n",
    "\n",
    "We train a simple logistic regression model on the synthetic data to get a\n",
    "quick sense of how separable churn vs. non-churn cases are. This is separate\n",
    "from the main XGBoost model used by the training script, but it gives a nice\n    ",
    "sanity check on the signal in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[FEATURE_COLUMNS]\n",
    "y = df[TARGET_COLUMN]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"Logistic regression ROC AUC: {auc:.3f}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, ax=ax[0])\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba, ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}